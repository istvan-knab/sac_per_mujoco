diff --git a/replay_memory/numpy_buffer/replay_memory.py b/replay_memory/numpy_buffer/replay_memory.py
index 82f7ed0..6466988 100644
--- a/replay_memory/numpy_buffer/replay_memory.py
+++ b/replay_memory/numpy_buffer/replay_memory.py
@@ -1,7 +1,4 @@
 import numpy as np
-import random
-import torch
-from numba import njit
 
 
 class ReplayMemory:
@@ -20,12 +17,6 @@ class ReplayMemory:
         self.indicies = np.zeros(self.batch_size)
 
     def add_element(self, state, action, next_state, reward, done):
-        state = state.cpu().numpy()
-        action = action.cpu().numpy()
-        next_state = next_state.cpu().numpy()
-        reward = reward.cpu().numpy()
-        done = done.cpu().numpy()
-
         if self.queue_length < self.buffer_size:
             self.queue_length += 1
         self.state = np.roll(self.state, (1, 0), (0, 1))
@@ -40,7 +31,7 @@ class ReplayMemory:
         self.done[0] = done
 
 
-    @njit
+
     def sample(self):
         self.indicies = np.random.choice(range(self.queue_length), self.batch_size)
         print("Hello")
diff --git a/train.py b/train.py
index 92643ab..85040c8 100644
--- a/train.py
+++ b/train.py
@@ -1,7 +1,7 @@
 import numpy as np
 import yaml
 import torch
-from torch.xpu import device
+from functorch.dim import Tensor
 from tqdm import tqdm
 import gymnasium as gym
 from collections import deque
@@ -43,7 +43,7 @@ def rl_loop():
     for episode in tqdm(range(config["EPISODES"]), desc='Training Process',
                         bar_format=logger.set_tqdm(), colour='white'):
         state = env.reset(config["SEED"])
-        done = torch.tensor(False).unsqueeze(0).unsqueeze(0)
+        done = np.array([False])
         episode_reward = 0
         episode_critic_1_loss = 0
         episode_critic_2_loss = 0
@@ -55,8 +55,9 @@ def rl_loop():
             action, _ = agent.actor.sample_action(state)
             next_state, reward, terminated, truncated, info = env.step(action)
             if terminated or truncated:
-                done = torch.tensor(True).unsqueeze(0).unsqueeze(0)
-            agent.memory.add_element(state, action, next_state, reward, done)
+                done = np.array([True])
+
+            agent.memory.add_element(state, np.array(Tensor.cpu(action)), next_state, reward, done)
             state = next_state
             episode_reward += reward
             critic_1_loss, critic_2_loss, actor_loss = agent.update_policy()
diff --git a/train_setup/config.yaml b/train_setup/config.yaml
index 3539be9..87bd463 100644
--- a/train_setup/config.yaml
+++ b/train_setup/config.yaml
@@ -18,7 +18,7 @@ ENTROPY_START: 0.1
 ENTROPY_END : 0.01
 DISCOUNT_FACTOR : 0.99
 BUFFER_SIZE: 5
-BATCH_SIZE : 512
+BATCH_SIZE : 3
 TAU : 0.05
 
 #Neural Network
